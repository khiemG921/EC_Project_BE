name: News Crawl and Import

on:
  schedule:
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      maxTotal:
        description: 'Max total rows to keep (prune oldest beyond this)'
        required: false
        default: '500'

jobs:
  crawl:
    runs-on: ubuntu-latest
    env:
      BACKEND_BASE_URL: ${{ secrets.BACKEND_BASE_URL }}
      CRON_SECRET: ${{ secrets.CRON_SECRET }}
      MAX_TOTAL: ${{ github.event.inputs.maxTotal || '500' }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Validate secrets
        run: |
          if [ -z "$BACKEND_BASE_URL" ] || [ -z "$CRON_SECRET" ]; then
            echo "Missing BACKEND_BASE_URL or CRON_SECRET" >&2
            exit 1
          fi
          echo "Using backend: $BACKEND_BASE_URL"

      - name: Trigger crawler run
        run: |
          set -e
          echo "Calling $BACKEND_BASE_URL/api/crawler/run"
          curl -sS -X POST "$BACKEND_BASE_URL/api/crawler/run" \
            -H "x-cron-key: $CRON_SECRET" -H "Accept: application/json" -w "\nHTTP_CODE:%{http_code}\n" --fail || (echo "Crawler trigger failed" && exit 1)

      - name: Import latest CSV into DB
        run: |
          set -e
          MT=${MAX_TOTAL:-500}
          echo "Importing with maxTotal=$MT"
          curl -sS -X POST "$BACKEND_BASE_URL/api/news-import/import-latest" \
            -H "x-cron-key: $CRON_SECRET" \
            -H "Content-Type: application/json" \
            -d "{\"maxTotal\": $MT}" -w "\nHTTP_CODE:%{http_code}\n" --fail